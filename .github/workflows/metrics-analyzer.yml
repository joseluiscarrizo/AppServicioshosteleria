name: Metrics Analyzer

on:
  schedule:
    # Run daily at 08:00 UTC
    - cron: '0 8 * * *'
  workflow_dispatch:

jobs:
  analyze-metrics:
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download latest metrics artifact
        id: download
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          pattern: workflow-metrics-*
          merge-multiple: true
          path: /tmp/artifacts

      - name: Analyze metrics and detect anomalies
        id: analysis
        run: |
          python3 - <<'PYEOF'
          import json, math, os, glob
          from datetime import datetime, timezone

          # Load all collected metrics
          artifact_files = glob.glob('/tmp/artifacts/*.json') + glob.glob('/tmp/artifacts/**/*.json', recursive=True)
          docs_data = '/home/runner/work/AppServicioshosteleria/AppServicioshosteleria/docs/data.json'
          if os.path.exists(docs_data):
              artifact_files.append(docs_data)

          all_runs = []
          for fp in artifact_files:
              try:
                  with open(fp) as f:
                      data = json.load(f)
                  # Support both raw metrics and the docs/data.json format
                  if 'recent_runs' in data:
                      all_runs.extend(data['recent_runs'])
                  elif 'latest' in data and 'recent_runs' in data.get('latest', {}):
                      all_runs.extend(data['latest']['recent_runs'])
                  elif 'history' in data:
                      for h in data['history']:
                          all_runs.extend(h.get('recent_runs', []))
              except Exception as e:
                  print(f"Skipping {fp}: {e}")

          # Deduplicate by run id
          seen = set()
          unique_runs = []
          for r in all_runs:
              rid = r.get('id')
              if rid and rid not in seen:
                  seen.add(rid)
                  unique_runs.append(r)

          total = len(unique_runs)
          successes = sum(1 for r in unique_runs if r.get('conclusion') == 'success')
          failures  = sum(1 for r in unique_runs if r.get('conclusion') == 'failure')

          durations = []
          for r in unique_runs:
              try:
                  s = datetime.fromisoformat(r['created_at'].replace('Z','+00:00'))
                  e = datetime.fromisoformat(r['updated_at'].replace('Z','+00:00'))
                  durations.append((e - s).total_seconds() * 1000)
              except Exception:
                  pass

          avg_ms = sum(durations) / len(durations) if durations else 0
          variance = sum((d - avg_ms)**2 for d in durations) / len(durations) if durations else 0
          std_dev = math.sqrt(variance)

          # Anomaly: duration > mean + 2*std_dev
          threshold = avg_ms + 2 * std_dev
          anomalies = [r for r, d in zip(unique_runs, durations) if d > threshold]

          success_rate = round(successes / total * 100, 2) if total else 0.0
          sla_ok = sum(1 for d in durations if d <= 300000)
          sla_compliance = round(sla_ok / len(durations) * 100, 2) if durations else 100.0
          sla_at_risk = sla_compliance < 95.0

          report = {
              "report_date": datetime.now(timezone.utc).isoformat(),
              "summary": {
                  "total_runs_analyzed": total,
                  "success_rate_pct": success_rate,
                  "failure_count": failures,
                  "sla_compliance_pct": sla_compliance,
                  "sla_at_risk": sla_at_risk,
                  "avg_execution_ms": round(avg_ms),
                  "std_dev_ms": round(std_dev),
                  "anomaly_threshold_ms": round(threshold)
              },
              "anomalies": [{"id": r.get("id"), "name": r.get("name"), "conclusion": r.get("conclusion")} for r in anomalies],
              "alert": {
                  "triggered": sla_at_risk or len(anomalies) > 0,
                  "reason": ("SLA at risk" if sla_at_risk else "") + (" | Anomalies detected" if anomalies else "")
              }
          }

          with open('/tmp/analysis_report.json', 'w') as f:
              json.dump(report, f, indent=2)

          print(json.dumps(report, indent=2))

          # Set outputs for downstream steps
          print(f"::notice::Success rate: {success_rate}%  SLA: {sla_compliance}%  Anomalies: {len(anomalies)}")
          if report['alert']['triggered']:
              print(f"::warning::Alert triggered – {report['alert']['reason']}")
          PYEOF

      - name: Upload analysis report artifact
        uses: actions/upload-artifact@v4
        with:
          name: metrics-analysis-${{ github.run_id }}
          path: /tmp/analysis_report.json
          retention-days: 90

      - name: Notify Slack on SLA risk or anomalies
        uses: ./.github/workflows/slack-notifier.yml
        if: ${{ false }}
        # Inline call not supported – see notification-scheduler.yml for orchestration
